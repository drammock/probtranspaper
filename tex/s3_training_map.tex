\subsection{Maximum {\em A Posteriori} Adaptation}
\label{sec:adaptation}

Training from PTs can be improved by starting from a multilingual ASR,
and adapting its parameters to PTs in the target language.  The
Bayesian framework for maximum {\em a posteriori} (MAP) estimation
has been widely applied to GMM and HMM parameter estimation problems
such as parameter smoothing and speaker
adaptation~\cite{gauvain1994maximum}.

Formally, for an unseen target language, denote its acoustic
observations $x = ( x_1^1, \ldots, x_{T}^L )$, and its acoustic model
parameter set as $\theta$, then the MAP parameters are defined as:
\begin{equation}
  \theta_{\mathrm{MAP}}  = \argmax_{\theta} \pi(\theta | x) 
= \argmax_{\theta} \pi( x | \theta ) \pi(\theta)
\label{eq:map}
\end{equation}
\noindent where $\pi(\theta)$ is the product of conjugate gradient
prior distributions, centered at the parameters of a cross-lingual
baseline GMM-HMM.  In a GMM-HMM, the acoustic model 
%\[
%\pi(x_t^\ell|s_t^\ell =j,\theta )=
%\sum_{k=1}^K c_{jk}
%\mathcal{N}
%\left(x_t^\ell|\mu_{jk},v_{jk}
%\right)
%\]
%\noindent
parameters $\theta=\left\{c_{jk},\mu_{jk},v_{jk}\right\}$ include
mixture weights, mean vectors, and variance vectors.  Maximum
likelihood trains these parameters by computing $\gamma_{\ell
  t}(j,k)=\pi(k|s_t^\ell =j,\phi^\ell,\theta)\pi(s_t^\ell
=j,\phi^\ell,\theta)$, then accumulating weighted average acoustic
frames with weights given by $\gamma_{\ell t}(j,k)$. Segmental K-means
quantizes $\pi(s_t^\ell
=j|\phi^\ell,\theta)\rightarrow\left\{0,1\right\}$ using forced
alignment, then proceeds identically.  MAP adaptation assigns, to each
parameter, a conjugate prior $\pi(\theta)$ with mode equal to
$\bar\theta$ (the parameters of the multilingual baseline), and with a
confidence hyperparameter $\tau_\theta$, resulting in re-estimation
formulae that are linearly interpolated between the baseline
parameters $\bar\theta$ and the statistics of the adaptation data, for
example:
\[
c_{jk}'=\frac{\tau_c\bar{c}_{jk}+\sum_{\ell,t}\gamma_{\ell t}(j,k)}
{\sum_{\kappa}\left(\tau_c\bar{c}_{j\kappa}+
  \sum_{\ell,t}\gamma_{\ell t}(j,\kappa)\right)},~~~
\mu_{jk}'=\frac{\tau_\mu\bar{\mu}_{jk}+\sum_{\ell,t}\gamma_{\ell t}(j,k)x_t^\ell}
   {\tau_\mu+\sum_{\ell,t}\gamma_{\ell t}(j,k)}
   %,~~~
%v_{jk}'=\frac{\tau_v\bar{v}_{jk}+\sum_{\ell,t}\gamma_{\ell t}(j,k)(x_t^\ell-\mu_{jk})^2}
%{\tau_v+\sum_{\ell,t}\gamma_{\ell t}(j,k)}
\]
%In our setting, the initial value of $\bar{\mu}_{jk}$ is obtained from
%the multilingual baseline model, and $\mu_{jk}'$ eventually converges
%to a model for the target language data.


