% Removed this section from discussion because I can't get Tyler's
% software to work yet... MH, 3/7/2016

In a sense, though, all of the results presented in this article, and
all results presented in every other article published on the subject
of under-resourced ASR, are artificial and disingenuous: ASR is
trained without deterministic transcripts, but is then tested by
comparing its output to a deterministic transcript.  In order to test
ASR in a language that truly lacks deterministic transcripts, it is
necessary to define an error metric that requires only PTs.  For
example, suppose we define MPER (minimum phone error rate) to be the
string edit distance between the ASR output and the closest-matching
string in the PT.  MPER is ill-defined: depending on how many
different types of speech perceptual errors are considered as
possibilities, it is possible to create a PT in which even the most
low-probability non-native perceptual error is still listed as a
possibility.  The best results are obtained by pruning the PT, thus if
${\tt 1best}:\mbox{FST}\rightarrow\mbox{FST}$ is an operator computing
the one-best path through an FST, ${\tt
  prune}:\mbox{FST},\Re\rightarrow\mbox{FST}$ is an operator that
prunes low-probability edges, and ${\bf E}$ is an FST computing
string-edit distance, then a potentially useful error metric can be
defined from PTs according to
\begin{equation}
  \mbox{MPER}(\beta) = \mathrm{cost}
  \left(\mathrm{1best}
  \left(\mathrm{1best}
  \left({\bf ASR}\right)
  \circ{\bf E}\circ\mathrm{prune}
  \left({\bf PT},\beta\right)
  \right)
  \right)
\end{equation}
where the pruning operation removes, from each slot $m$, any phone
whose negative log probability $-\ln\rho(\phi_m^\ell)$ is higher than
the minimum-cost path by a difference greater than $\beta$:
\begin{equation}
-\ln\hat{\rho}_{\Phi_m^\ell}(k) = \left\{\begin{array}{ll}
0 & \mbox{if}~\ln\max_j \left(\frac{\rho_{\Phi_m^\ell}(j)}
    {\rho_{\Phi_m^\ell}(k)}\right) < \beta \\
    \infty & \mbox{otherwise}
    \end{array}\right.
\label{eq:pper}
\end{equation}
Three candidate measures are shown in Table~\ref{tab:pper}.  Each
column shows the difference between PER or MPER of two different
systems: a multilingual ASR (column {\sc MULT-L} in
Table~\ref{tab:ptresult}, called $\mbox{PER}_M$ or $\mbox{MPER}_M$)
and a PT-adapted ASR (column {\sc PT-adapt} in
Table~\ref{tab:ptresult}, called $\mbox{PER}_A$ or $\mbox{MPER}_A$).
The second row is true PER, if known; we were unable to hire a
Japanese transcriber, so no true PERs are available in the last column
of the table.  Remaining rows show MPER defined as the PER between the
ASR output and the closest-matching path through the PT phone lattice.
$\mbox{MPER}(1000)$ performs very little pruning, and is therefore too
small to be useful; $\mbox{MPER}(0.1)$ usually prunes away all paths
except the best path, and is therefore too sensitive to label noise in
the PT.  Of the measures tested, $\mbox{MPER}(1)$ seems to correlate
best with the true differences between PERs of multilingual and
PT-adapted systems, though of course it is not possible to measure
significance of the correlation with only three data points.

\begin{table}
  \centerline{\begin{tabular}{|l|cccc|}\hline
      Error Metric & hun & cmn & swh & jap \\ \hline\hline
      $\mbox{PER}_M$
      & 66.90-57.26 & 68.66-57.85 & 64.73-46.88 & \\
      $-\mbox{PER}_A$
      & =9.64 & =10.81 & =15.85 & \\\hline
      $\mbox{MPER}_A(0.1)$
      &&&& \\
      $-\mbox{MPER}_M(0.1)$
      &&&& \\\hline
      $\mbox{MPER}_A(1)$
      &&&& \\
      $-\mbox{MPER}_M(1)$
      &&&& \\\hline
      $\mbox{MPER}_A(1000)$
      &&&& \\
      $-\mbox{MPER}_M(1000)$
      &&&& \\\hline
  \end{tabular}}
  \caption{PER computed using a native transcription (where
    available), and minimum PER computed using probabilistic
    transcriptions.}
  \label{tab:pper}
\end{table}
    
