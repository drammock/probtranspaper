\subsection{Channel Model from EEG}

Mismatched crowdsourcing depends on a misperception G2P, which can be
trained from doubly-transcribed data in languages other than the test
language, as described in the previous section.  With a small amount
of transcribed data in the utterance language, however, it is possible
to estimate the misperception G2P using electrocortical measurements
of non-native speech perception, as follows.

The misperception G2P can be decomposed into two separate transducers,
a misperception transducer $\rho(\psi|\phi)$, and an
annotation-language G2P $\rho(\lambda|\psi)$:
\begin{equation}
  \Pr(\lambda|\psi)=\sum_{\psi}\Pr(\lambda|\psi)\Pr(\psi|\phi)
\end{equation}
where $\phi$ is a phone string in the utterance language, $\psi$ is a
phone string in the annotation language, and $\lambda$ is an
orthographic string in the annotation language.  $\rho(\lambda|\psi)$
is an inverted G2P in the annotation language, e.g., trained on the
CMU dictionary of American English pronunciations~\cite{Lenzo1995}.
$\rho(\psi|\phi)$ is the mismatch transducer, specifying the
probability that a phone string $\phi$ in the utterance language will
be mis-heard as the annotation-language phone string $\psi$.

Distinctive features were proposed to characterize the perceptual and
phonological natural classes of phonemes~\cite{Jakobson52}, therefore
the distance between the distinctive feature representations of two
phonemes is an indirect predictor of their confusion probability.  Let
$f_k(\phi)$ be the $k^{\textrm{th}}$ feature of phoneme $\phi$, and
let $\delta(\cdot)\in\left\{0,1\right\}$ be the unit indicator
function.  The assumption that every distinctive feature shared by
phonemes $\phi$ and $\psi$ independently increases their confusion
probability can be expressed as
\begin{equation}
  \rho(\psi|\phi)\propto \exp\left(-\sum_{k=1}^K
  w_k\delta\left(f_k(\psi)\ne f_k(\phi)\right)\right)
  \label{eq:dfdist}
\end{equation}
The weights $w_k$ probably depend on the listener, and on the
identities of both speaker language and listener language, but data to
train such a rich model do not exist; a reasonable approximate model
can be learned by assuming that $w_k$ depend only on the language of
the listener.

Distinctive features were defined according to the PHOIBLE inventory.
The feature weights, $w_k$, were set differently in different
experiments.  In some experiments, they were set to be uniform.
Better accuracy was obtained, however, by setting them on the basis of
perceptual similarity as measured using EEG.

Denote as $y(\psi)$ the set of EEG signals recorded when a listener
hears audio corresponding to phoneme $\psi$ in the annotation
language, and suppose that $g_k(y(\psi))$ is the output of a binary
classifier of EEG signals trained to label the $k^{\textrm{th}}$
distinctive feature of phoneme $\psi$, as in~\cite{Liberto15}.  Then
the weights in Eq.~\ref{eq:dfdist} can be estimated as
\begin{equation}
  w_k = -\ln\Pr\left\{g_k(y(\psi))\ne f_k(\psi)\right\}
  \label{eq:eegdist}
\end{equation}

