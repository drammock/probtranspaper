\subsection{Cross-Lingual Baseline}
\label{s6:mlbaseline}

Table~\ref{tab:results} compares results using universal phone set and
phone language model to those obtained using language-dependent phone
set and phone language model.  Without a language specific phone set
and phone language model, it is hard for a cross-lingual system to
generalize to an unseen language.  This is true even if the system has
seen closely related languages such as Mandarin when tested on
Cantonese.  

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|cccc|}
\hline
data & acoustic & language & yue & hun & cmn & swh \\
 & model & model &  & & & \\
\hline
cross-lingual & GMM & cross-lingual & 79.64 (79.83) & 77.13 (77.85) & 83.28 (82.12) & 82.99 (81.86) \\
cross-lingual & NN & cross-lingual & 78.62 (77.58) & 75.98 (76.44) & 81.86 (80.47) & 82.30 (81.18) \\
cross-lingual & GMM & text & 68.40 (68.35) & 68.62 (66.90) & 71.30 (68.66) & 63.04 (64.73) \\
cross-lingual & NN & text & 66.54 (65.28) & 66.08 (66.58) & 65.77 (64.80) & 64.75 (65.04) \\
\hline
monolingual & GMM & transcript & 32.77 (34.61) & 39.58 (39.77) & 32.21 (26.92) & 35.33 (46.51) \\
monolingual & NN & transcript & 27.67 (28.88) & 35.87 (36.58) & 27.80 (23.96) & 34.98 (41.47) \\
\hline
\end{tabular}
\caption{\label{tab:results} PERs of unadapted cross-lingual systems on
  the evaluation sets along with monolingual systems.  PERs on the
  development sets are in parentheses.  Text-based language models are
  trained using phone sequences computed by applying a G2P to
  independent Wikipedia texts in the target language. Transcript-based
  language models are trained using phone sequences computed by
  applying a G2P to native transcripts of the training data.}
\end{center}
\end{table*}

From the comparison of different baseline systems, we can reach the
following conclusions.  First, even with only 40 minutes of
training data, a NN is able to outperform a GMM.  Second,
however, the standard speech pipeline generalizes well to languages
that were seen in the training corpus, but performs poorly on unseen
languages.  Using a language-specific phonotactic language model gives
significant improvement over the language-independent phonotactic
model, but nevertheless significantly underperforms a system that
has seen the test language during training.  

