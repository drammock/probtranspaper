\subsection{Cross-Lingual Baseline}
\label{s6:mlbaseline}

Table~\ref{tab:results} compares cross-lingual ASR using universal phone set and
phone language model, cross-lingual ASR using language-dependent phone
set and phone language model, and monolingual ASR.
%Without a language specific phone set
%and phone language model, it is hard for a cross-lingual system to
%generalize to an unseen language.  This is true even if the system has
%seen closely related languages such as Mandarin when tested on
%Cantonese.  

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|cccc|}
\hline
data & acoustic & language & yue & hun & cmn & swh \\
 & model & model &  & & & \\
\hline
cross-lingual & GMM & cross-lingual & 79.64 (79.83) & 77.13 (77.85) & 83.28 (82.12) & 82.99 (81.86) \\
cross-lingual & NN & cross-lingual & 78.62 (77.58) & 75.98 (76.44) & 81.86 (80.47) & 82.30 (81.18) \\
cross-lingual & GMM & text & 68.40 (68.35) & 68.62 (66.90) & 71.30 (68.66) & 63.04 (64.73) \\
cross-lingual & NN & text & 66.54 (65.28) & 66.08 (66.58) & 65.77 (64.80) & 64.75 (65.04) \\
\hline
monolingual & GMM & transcript & 32.77 (34.61) & 39.58 (39.77) & 32.21 (26.92) & 35.33 (46.51) \\
monolingual & NN & transcript & 27.67 (28.88) & 35.87 (36.58) & 27.80 (23.96) & 34.98 (41.47) \\
\hline
\end{tabular}
\caption{\label{tab:results} PERs of unadapted cross-lingual
  and monolingual ASR on
  the evaluation sets (development sets are in parentheses).
  Text-based language models are
  trained using Wikipedia.
  Transcript-based
  language models are based on
  native transcripts of the training data.}
\end{center}
\end{table*}

From the comparison of different baseline systems, we can reach the
following conclusions.  First, even with only 40 minutes of
training data, a NN is able to outperform a GMM.  Second,
however, the standard speech pipeline performs poorly on unseen
languages.  Using a language-specific phonotactic language model gives
significant improvement over the language-independent phonotactic
model, but nevertheless significantly underperforms a system that
has seen the test language during training.  This is true
even if the system has seen closely related languages during training:
the Cantonese cross-lingual system has seen Mandarin during training,
and the Mandarin system has seen Cantonese during training, but
neither system is able to generalize well from its training language to
its test language.

