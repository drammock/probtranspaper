\subsection{EEG Recording and Analysis}
\label{sec:methods_eeg}

To compute distinctive feature weights used in estimating the misperception
transducer as shown in Eqs.~(\ref{eq:dfdist}) and~(\ref{eq:eegdist}),
recordings of cortical activity in response to non-native phones were
made using EEG. Signals were acquired using a BrainVision actiCHamp 
system with 64 channels and 1000 Hz sampling rate.
All methods were approved by the University of Washington Institutional
Review Board.

Auditory stimuli used to evoke the electrocortical responses comprised
consonant-vowel (CV) syllables representing three languages: English,
Dutch and Hindi. The inclusion of only two non-English languages in the
auditory stimuli was dictated by the relatively high number of
repetitions required to achieve good signal-to-noise ratio from averaged
EEG recordings. The choice of Dutch and Hindi was governed by their
inclusion in the SBS subset used to train the misperception G2P as
described in Section~\ref{sec:MC}, and the relative similarities
between their phoneme inventories and the phoneme inventory of English.
In particular, we chose languages with relatively many (Hindi) or few
(Dutch) ``many-to-one mappings'' between the non-English phoneme
inventory and the phoneme inventory of English, estimated based on 
distinctive feature representations of the phonemes in each language 
(as given in the PHOIBLE database~\cite{PHOIBLE}). Such 
``many-to-one mappings'' are expected to pose a problem for the 
non-native transcription task being modeled by the misperception 
transducer, so to test the limits of our design we chose languages that 
differed greatly in this property. 
%% rest of this paragraph could probably be cut if necessary
Note that, although Hindi podcasts were not included in the SBS training
data described in Section~\ref{sec:data}, colloquial spoken Hindi and
Urdu are extremely similar phonologically~\cite{Kachru90}, and
considering that the auditory stimuli for the EEG portion of this
experiment are simple CV syllables, it is reasonable to consider Hindi
and Urdu as equivalent for the purpose of computing feature weights for
the misperception transducer.

%% This next section, including tab:m2o, seems like more detail than
%% is necessary; the explanation above as to **why** we chose Hindi and
%% Dutch (combined with tab:eegphones) seems sufficient here, without
%% going into the extra details of **how** we settled on those two.
% MH: I feel like it's useful to include at least some of this information.
% I could imagine different ways to define the ``number of many-to-one
% mappings,'' other than the metric listed in eq:m2o.
% Somebody someday might come back and test different versions of this
% metric to see which one is most useful, for some definition of ``useful.''
Language similarity was defined as the number of many-to-one mappings
($N_{M2O}(\Omega_\Phi)$) between the English phoneme inventory
($\Omega_\Psi$) and the non-native phoneme inventory $\Omega_\Phi$.
Using distinctive feature representations of the phonemes in each
inventory (as given in the PHOIBLE database \cite{PHOIBLE}), a
many-to-one mapping was defined by finding, for each
non-native phoneme $\phi$, the English phoneme $\psi^*(\phi)$ to which
it is most similar:
\begin{equation}
  \psi^*(\phi) = \argmin \sum_k \delta\left(f_k(\psi)\ne f_k(\phi)\right)
\end{equation}
The number of many-to-one collisions is then defined as
\begin{equation}
  N_{M2O}(\Omega_\Phi)=\frac{1}{|\Omega_\Psi|}\sum_{\phi_1\ne\phi_2}
  \delta\left(\psi^*(\phi_1)=\psi^*(\phi_2)\right)
\label{eq:m2o}
\end{equation}
where $|\Omega_\Psi|$ is the size of the English phoneme inventory.
The frequency of many-to-one mappings is listed in
Table~\ref{tab:m2o} for several languages.

\begin{table}
  \centerline{\begin{tabular}{|lr|lr|lr|}\hline
    $\Omega_\Phi$ & $N_{M2O}(\Omega_\Phi)$ &
    $\Omega_\Phi$ & $N_{M2O}(\Omega_\Phi)$ &
    $\Omega_\Phi$ & $N_{M2O}(\Omega_\Phi)$ \\ \hline
    spa & 0.862 & yue & 1.280 & cmn & 1.531 \\
    por & 1.152 & jpn & 1.333 & amh & 1.844 \\
    nld & 1.182 & vie & 1.393 & hun & 1.857 \\
    deu & 1.258 & kor & 1.429 & hin & 2.848 \\\hline
  \end{tabular}}
  \caption{Frequency of many-to-one mappings $N_{M2O}(\Omega_\Phi)$
    between phoneme inventory $\Omega_\Phi$ and the inventory of
    English. Languages are represented by their ISO 639-3 codes.}
  \label{tab:m2o}
\end{table}

To construct the auditory stimuli, two vowels and several consonants
were selected from the phoneme inventory of each language (18 consonants
for English, 17 for Dutch, and 19 for Hindi). Choice of consonants was
made so as to emphasize differences in the many-to-one relationships
between English-Dutch and English-Hindi, while maintaining roughly equal 
numbers of consonants for each language. The consonants chosen for each 
language are given in Table~\ref{tab:eegphones}; the vowels chosen were 
the same for all three languages (/a/ and /e/).

\begin{table}
  \centering
  \setlength{\tabcolsep}{0.3em}
  \setlength\extrarowheight{2pt}
  \begin{tabular}{|l||cc|cccc|cc|c|cccc|cc|c|c|c|c|c|c|c|c|c|cc|c|c|c|}\hline
    Language & \multicolumn{29}{c|}{Consonant phones used in the EEG experiment}\\ \hline
    eng & \multicolumn{2}{c|}{p} & \multicolumn{4}{c|}{t} & \multicolumn{2}{c|}{k} & \textipa{p\super h} & \multicolumn{4}{c|}{\textipa{t\super h}} & \multicolumn{2}{c|}{\textipa{k\super h}} & \textipa{tS} & \textipa{tS\super h} & f & \textipa{T} & \textipa{S} & v & \textipa{D} & z & m & \multicolumn{2}{c|}{n} & l & \textipa{\*r} & \\ \hline
    nld &  \multicolumn{2}{c|}{p} & \multicolumn{4}{c|}{t} & \multicolumn{2}{c|}{\textipa{G}} & \textipa{p\super h} & \multicolumn{4}{c|}{\textipa{t\super h}} & \multicolumn{2}{c|}{\textipa{k\super h}} & & \textipa{tS\super h} & f & & \textipa{S} & v & & z & m & \multicolumn{2}{c|}{n} & l & \textipa{\;R} & j \\ \hline
    hin &  p & b & \textipa{\|[t} & \textipa{\|[d} & \textipa{\:t} & \textipa{\:d} & k & \textipa{g} & \textipa{b\super H} & \textipa{\|[t\super h} & \textipa{\:t\super h} & \textipa{\|[d\super H} & \textipa{\:d\super H} & \textipa{k\super h} & \textipa{g\super H} & & & & & & \textipa{V} & & & m & \textipa{\|[n} & \textipa{\:n} & & & \\ \hline
  \end{tabular}
  \caption{Consonant phones used in the EEG experiment represented using
  IPA. Vertical alignment of cells suggests many-to-one mappings
  expected based on distinctive feature values from PHOIBLE.}
  \label{tab:eegphones}
\end{table}

Two native speakers of each language (one male and one female) were
recorded (44100 sample rate, 16 bit depth) speaking multiple repetitions of the set of CV syllables for
their language. Three tokens of each unique syllable were excised from
the raw recordings (24414 downsampled, RMS normalized)
Recorded syllables had an average duration of 400~ms, and were presented
via headphones to one monolingual American English listener.
The stimuli were presented in 9 blocks of 15 minutes per block, for a
total of 135 minutes.  Syllables were presented in random order with an
inter-stimulus interval of 350~ms. 21 repetitions of each syllable
were presented, for a grand total of 9072 syllable presentations.

%% TODO: get number of ms where epoch started (MM to email GDL)
EEG recordings were divided into 500 ms epochs.
The epoched data were coded with a subset of distinctive features
that minimally defined the phoneme contrasts of the English consonants.
Where more than one choice of features was sufficient to define those
contrasts, preference was given to features that reflect differences
in temporal as opposed to spectral features of the consonants, due to
the high fidelity of EEG at reflecting temporal envelope properties of 
speech.~\cite{Liberto15} The final set of features chosen was:
continuant, sonorant, delayed release, voicing, aspiration, labial,
coronal, and dorsal.
% In other words, differences in the temporal amplitude envelope of
% consonants have a better chance of being recoverable after being
% filtered through a human auditory system and cortex than do differences
% that are purely spectral in nature; to the extent that spectral
% information in speech is preserved in an EEG signal, it will have been
% transformed to be spatially coded across populations of neurons.

